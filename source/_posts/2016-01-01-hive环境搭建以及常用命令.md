date: "2016-01-02"
categories: 
  - 大数据
title: Hive环境搭建以及常用命令
tags : 
 - hadoop
 - hive
---


 Hadoop版本为2.6.0 ，参考下面安装

 [Setting up a Single Node Cluster](http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Purpose)

 发现还是官网的靠谱，按照操作能正常运行hadoop2.6.0

 不过环境搭建完了之后，不知道下一步干啥，因为在目前的工作中，主要用的是Hive，所以在也把Hive环境搭建起来


 Hive版本为1.2.1，参考下面安装

 [GettingStarted](https://cwiki.apache.org/confluence/display/Hive/GettingStarted)


 如果出现以下错误

```sh

Terminal initialization failed; falling back to unsupported
java.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected
        at jline.TerminalFactory.create(TerminalFactory.java:101)
        at jline.TerminalFactory.get(TerminalFactory.java:158)
        at jline.console.ConsoleReader.<init>(ConsoleReader.java:229)
        at jline.console.ConsoleReader.<init>(ConsoleReader.java:221)
        at jline.console.ConsoleReader.<init>(ConsoleReader.java:209)
        at org.apache.hadoop.hive.cli.CliDriver.getConsoleReader(CliDriver.java:773)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:715)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)

```

原因是因为hadoop目录下村镇老版本jline-0.9.94.jar在hadoop-2.6.0/share/hadoop/yarn/lib下

将该jar文件删除，用hive中的jline-2.12.jar替换

# 常用命令

参考[hive创建/删除/截断 表](http://www.cnblogs.com/ggjucheng/archive/2013/01/04/2844393.html)有改动


## 登陆

$HIVE_HOME/bin/hive  启动

## 显示数据库和选择数据库

```sql
 
 show databases;

 use default;

```

## 创建表

```sql

create table t_hive1(
	id  int,
	dtDontQuery  string,
	name  string
);

```

## 创建有分区的表

```sql

create table  t_hive2(
	id int  comment '用户id',
	dtDontQuery  string comment '不知道是什么',
	name   string  comment '用户名称'
)
partitioned by (stat_day string)  #字段名称不能使用date等关键字
;

```


## 典型的默认表创建

```sql

create table page_view(
	view_time int comment '浏览时间',
	userid bigint  comment '用户id',
	page_url string  comment '浏览的url',
	referrer_url string  comment 'refer',
	ip string comment  '用户ip'
)
comment '这是一个pv表'
partitioned by (dt string, country string)
row format delimited
  fields terminated by '\001'
  collection items terminated  by '\002'
  map keys terminated  by '\003'
stored as textfile  
;

```

row  format  delimited  是用来设置创建的表在加载数据的时候，支持的列分割符。不同列之间用一个'\001'分割，集合(例如 array,map)的元素之间以'\002'隔开，map中的key和value用'\003'分割

## 创建外部表

如果数据已经存在hsfs的'/Users/liuyiyou/hadoop-2.6.0/warehouse/page_view'上了，如果想创建表，指向这个路径，就需要创建外部表

```sql

  #这个没有试验，hadoop很多概念还不懂呢！

```

## 从文件加载到Hive表

参考[hive数据操作](http://www.cnblogs.com/ggjucheng/archive/2013/01/04/2844673.html)

加载数据到表时，hive不会做任何转换。加载操作是纯粹的复制/移动操作，移动数据文件到相应的hive表。

LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]

```sql

load data local inpath  '/Users/liuyiyou/apache-hive-1.2.1-bin/examples/files/kv3.txt' overwrite into table invites partition(ds='2008-08-08');;

```


